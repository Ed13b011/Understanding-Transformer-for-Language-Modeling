{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformers1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xAKBIhuDJ3v2","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UtYK_SFWMQgH","colab_type":"code","colab":{}},"source":["# embedding \n","class Embedder(nn.Module):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.embed = nn.Embedding(vocab_size, d_model)\n","  def forward(self, x):\n","    returnself.embed(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWr4onKWfZWj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"LGR_oylcQHUq","colab_type":"code","colab":{}},"source":["# positional encodings\n","class PositionalEncoder(nn.Module):\n","  def __init__(self, d_model, max_seq_len = 80):\n","    super().__init__()\n","    self.d_model = d_model\n","\n","    # create constant 'pe' matrix with values dependent on pos and i\n","    # pos refers to order in the sentence\n","    # i refers to the position along the embedding vector dimension\n","    pe = torch.zeros(max_seq_len, d_model) # pe-> 2d matrix\n","    for pos in range(max_seq_len):\n","      for i in range(0, d_model, 2):\n","        pe[pos, i] = \\\n","        math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","        pe[pos, i + 1] = \\\n","        math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","\n","    pe = pe.unsqueeze(0)\n","    self.register_buffer('pe', pe)\n","\n","  def forward(self, x):\n","    # make emedding relatively larger\n","    x = x * math.sqrt(slef.d_model)\n","    # add constant to embedding\n","    seq_len = x.size(1)\n","    x = x + Variable(self.pe[:,:seq_len], \\\n","    requires_grad=False).cuda()\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09KOEHVHefAk","colab_type":"text"},"source":["The reason we increase the embedding values before addition is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector wonâ€™t be lost when we add them together."]},{"cell_type":"code","metadata":{"id":"I3-2z0hleWQr","colab_type":"code","colab":{}},"source":["# creating mask for the input\n","batch = next(iter(train_iter))\n","input_seq = batch.English.transpose(0,1)\n","input_pad = EN_TEXT.vocab.stoi['<pad>']\n","\n","# creates mask with 0s wherever there is padding in the input\n","input_msk = (input_seq != input_pad).unsqueeze(1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZe_g0Vkj2dI","colab_type":"code","colab":{}},"source":["# create mask for the target_seq \n","\n","# create mask as before\n","\n","target_seq = batch.French.transpose(0,1)\n","target_pad = FR_TEXT.vocab.stoi['<pad>']\n","target_msk = (target_seq != target_pad).unsqueeze(1)\n","\n","size = target_seq.size(1) # get seq_len for matrix\n","\n","# nopeak_mask-> we need to prevent the first output predicitions from being able to see later into the sentence\n","nopeak_mask = np.triu(np.ones(1, size, size), k=1).astype('uint8')\n","nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n","\n","target_msk = target_msk & nopeak_mask"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCbXpwjPmnnK","colab_type":"text"},"source":["If we later apply this mask to the attention scores, the values wherever the input is ahead will not be able to contribute when calculating the outputs"]},{"cell_type":"markdown","metadata":{"id":"UpyrmghNp59S","colab_type":"text"},"source":["Multi-Head Attention"]},{"cell_type":"code","metadata":{"id":"_m6hVMmGmdFh","colab_type":"code","colab":{}},"source":["# decoder module\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, heads, d_model, dropout = 0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.d_k = d_model // heads # d_model/N, N is number of heads\n","    self.h = heads\n","\n","    self.q_linear = nn.Linear(d_model, d_model)\n","    self.v_linear = nn.Linear(d_model, d_model)\n","    self.k_linear = nn.Linear(d_model, d_model)\n","    self.dropout = nn.Dropout(dropout)\n","    self.out = nn.Linear(d_model, d_model)\n","\n","  def forward(self, q, k, v, mask=None):\n","    bs = q.size(0)\n","\n","    # perform linear operation and split into h heads\n","    k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n","    q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n","    v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n","\n","    # transpose to get dimension bs * h * s1 * d_model\n","    k = k.transpose(1,2)\n","    q = q.transpose(1,2)\n","    v = v.transpose(1,2)\n","\n","    # calculate the attention using function  we will define next\n","    scores = attention(q, k, v, self.d_k, slef.dropout)\n","\n","    # concatenate heads and put through final linear layer\n","    concat = scores.transpose(1,2).contiguous()\\\n","    .view(bs, -1, self.d_model)\n","\n","    output = slef.out(concat)\n","\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sAGFmqxtvdA","colab_type":"code","colab":{}},"source":["# attention function\n","def attention(q, k, v, d_k, mask=None, dropout=None):\n","  scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","\n","  if mask is not None:\n","    mask = mask.unsqueeze(1)\n","    scores = scores.masked_fill(mask == 0, -1e9)\n","\n","  scores = F.softmax(scores, dim=-1)\n","\n","  if dropout is not None:\n","    scores = dropout(scores)\n","\n","  output = torch.matmul(scores, v)\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQ8IHc7C0G88","colab_type":"text"},"source":["Feed Forward Network: the feed forward layer simply deepens our network, employing linear layers to analyse patterns in the attention layers output"]},{"cell_type":"code","metadata":{"id":"lKTw9aVhxxcT","colab_type":"code","colab":{}},"source":["# Feed-Forward Network (final layer)\n","# relu and dropout two linear operations\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n","    super().__init__()\n","    # we set d_ff as a default to 2048\n","    self.linear_1 = nn.Linear(d_model, d_ff)\n","    self.dropout = nn.Dropout(dropout)\n","    self.linear_2 = nn.Linear(d_ff, d_model)\n","  def forward(self, x):\n","    x = self.dropout(F.relu(self.linear_1(x)))\n","    x = self.linear_2(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmeCG1DS0BFi","colab_type":"code","colab":{}},"source":["# Normalization\n","\n","class Norm(nn.Module):\n","  def __init__(self, d_model, eps = 1e-6):\n","    super().__init__()\n","\n","    self.size = d_model\n","    # create two learnable parameters to calibrate normalisation\n","    self.alpha = nn.Parameter(torch.ones(self.size))\n","    self.bias = nn.Parameter(torch.zeros(self.size))\n","    self.eps = eps\n","  def forward(self, x):\n","    norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n","    / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n","    return norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFsSYOM43MUH","colab_type":"code","colab":{}},"source":["# now build  Encoder Layer and Decoder Layer modules with architecture (shown in paper)\n","\n","# # Encoder layer with one multi head attention layer and one feed forward layer\n","class EncoderLayer(nn.Module):\n","  def __init__(self, d_model, heads, dropout = 0.1):\n","    super().__init__()\n","    self.norm_1 = Norm(d_model) \n","    self.norm_2 = Norm(d_model) # two add & norm present in in Encoder layer\n","    self.attn = MultiHeadAttention(heads, d_model)\n","    self.ff = FeedForward(d_model)\n","    self.dropout_1 = nn.Dropout(dropout)\n","    self.dropout_2 = nn.Dropout(dropout)\n","\n","  def forward(self, x, mask):\n","    x2 = self.norm_1(x) # normalization\n","    x = x + self.dropout_1(self.attn(x2,x2,x2,mask)) # multi-head attention\n","    x2 = self.norm_2(x) # normalization\n","    x = x + self.dropout_2(self.ff(x2)) # feed forward network\n","    return x\n","\n","\n","# # bulid decoder layer with two multi head attention and one feed forward layer\n","class DecoderLayer(nn.Module):\n","  def __init__(self, d_model, heads, dropout = 0.1):\n","    super().__init__()\n","    self.norm_1 = Norm(d_model) # total 3 normalization layers\n","    self.norm_2 = Norm(d_model)\n","    self.norm_3 = Norm(d_model)\n","\n","    self.dropout_1 = nn.Dropout(dropout)\n","    self.dropout_2 = nn.Dropout(dropout)\n","    self.dropout_3 = nn.Dropout(dropout)\n","\n","    self.attn_1 = MultiHeadAttention(heads, d_model)\n","    self.attn_2 = MultiHeadAttention(heads, d_model)\n","    self.ff = FeedForward(d_model).cuda()\n","\n","  def forward(self, x, e_outputs, src_mask, trg_mask):\n","    x2 = self.norm_1(x)\n","    x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n","    x2 = self.norm_2(x)\n","    x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n","    x2 = self.norm_3(x)\n","    x = x + self.dropout_3(self.ff(x2))\n","    return x\n","\n","\n","# we can define then build a convenient cloning function that can generate multiple layers\n","\n","def get_clones(module, N):\n","  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZ5s5r_mANvS","colab_type":"code","colab":{}},"source":["# bulid Encoder and Decoder\n","\n","## encoder\n","class Encoder(nn.Module):\n","  def __init__(self, vocab_size, d_model, N, heads):\n","    super().__init__()\n","    self.N = N\n","    self.embed = Embedder(vocab_size, d_model)\n","    self.pe = PositionalEncoder(d_model)\n","    self.layers = get_clones(EncoderLayer(d_model, heads), N)\n","    self.norm = Norm(d_model)\n","\n","  def forward(self, src, mask):\n","    x = self.embed(src)\n","    x = self.pe(x)\n","    for i in range(N):\n","      x = self.layers[i](x, mask)\n","    return self.norm(x)\n","\n","## deocder\n","class Decoder(nn.Module):\n","  def __init__(self, vocab_size, d_model, N, heads):\n","    super().__init__()\n","    self.N = N\n","    self.embed = Embedder(vocab_size, d_model)\n","    self.pe = PositionalEncoder(d_model)\n","    self.layers = get_clones(DecoderLayer(d_model, heads), N)\n","    self.norm = Norm(d_model)\n","\n","  def forward(self, trg, e_outputs, src_mask, trg_mask):\n","    x = self.embed(trg)\n","    x = self.pe(x)\n","    for i in range(self.N):\n","      x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n","    return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMQ_wLkHR20j","colab_type":"code","colab":{}},"source":["# finally Transformer \n","class Transformer(nn.Module):\n","  def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n","    super().__init__()\n","    self.encoder = Encoder(src_vocab, d_model, N, heads)\n","    self.decoder = Decoder(trg_vocab, d_model, N, heads)\n","    self.out = nn.Linear(d_model, trg_vocab)\n","\n","  def forward(self, src, trg, src_mask, trg_mask):\n","    e_outputs = self.encoder(src, src_mask)\n","    d_outputs = self.decoder(trg, e_outputs, src_mask, trg_mask)\n","    output = self.out(d_outputs)\n","    return output\n","\n","# we don't perform softmax on the output as this will be handled\n","# automatically by our loss function\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJP0LOtQVdbb","colab_type":"code","colab":{}},"source":["# training our model\n","\n","## define some parameters\n","d_model = 512\n","heads = 8\n","N = 6\n","src_vocab = len(EN_TEXT.vocab)\n","trg_vocab = len(FR_TEXT.vocab)\n","\n","model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n","\n","for p in model.parameters():\n","  if p.dim() > 1:\n","    nn.init.xavier_uniform_(p)\n","\n","# this code is very important! It initialises the parameters with a\n","# range of values that stops the signal fading or getting too big.\n","\n","otim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9,0.98), eps=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fVg1MgsX19R","colab_type":"code","colab":{}},"source":["# training\n","def train_model(epochs, print_every=100):\n","  model.train()\n","  start = time.time()\n","  temp = start\n","\n","  total_loss = 0\n","\n","  for epoch in range(epochs):\n","    for i, batch in enumerate(train_iter):\n","      src = batch.English.transpose(0,1)\n","      trg = batch.French.transpose(0,1)\n","\n","      trg_input = trg[:, :-1]\n","\n","      targets = trg[:, 1:].contiguous().view(-1)\n","      # create function to make masks using mask code above\n","\n","      src_mask, trg_mask = create_mask(src, trg_input)\n","\n","      preds = model(src, trg_input, src_mask, trg_mask)\n","\n","      optim.zero_grad()\n","      loss = F.cross_entropy(preds.view(-1, preds.size(-1)),\n","                             results, ignore_index=target_pad)\n","      loss.backward()\n","      optim.step()\n","            \n","      total_loss += loss.data[0]\n","      \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnItaj5nX2O7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Utte4YiX2gi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}